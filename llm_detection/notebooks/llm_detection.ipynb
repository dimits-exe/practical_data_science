{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3adb1b-0f13-4bd5-8dde-e62b4afe50d5",
   "metadata": {},
   "source": [
    "# LLM detection\n",
    "## Practical Data Science course, MSc in Data Science (2023/2024)\n",
    "###  Assignment #3\n",
    "\n",
    "---\n",
    "\n",
    "> Dimitris Tsirmpas <br>\n",
    "> MSc in Data Science f3352315 <br>\n",
    "> Athens University of Economics and Business"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b6df4-7789-4986-adc2-8e01a4638ba7",
   "metadata": {},
   "source": [
    "## Disclaimers\n",
    "\n",
    "- Most documentation was generated by ChatGPT, and was manually corrected / augmented where necessary.\n",
    "- In each case where code has been obtained from outside sources, is clearly listed either in comments or in the markdown explaining the code block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9438e4c-51bd-4571-8fa6-f149e8da7d53",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea8502-d773-4151-990e-73ebd811fe94",
   "metadata": {},
   "source": [
    "We begin by reviewing the data provided to us by the competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2fed8d-eb1a-47fe-93bf-45181b8d4e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29fb4f7b6ae414ca9976644883fc262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm.auto import tqdm\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# file path module\n",
    "import notebook_config\n",
    "\n",
    "\n",
    "# enable progress bar functionality\n",
    "tqdm_notebook().pandas()\n",
    "start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9591a6-74da-423d-9e85-eb0252be86fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>fe6ff9a5</td>\n",
       "      <td>1</td>\n",
       "      <td>There has been a fuss about the Elector Colleg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>ff669174</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage has many advantages. Such a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>ffa247e0</td>\n",
       "      <td>0</td>\n",
       "      <td>There's a new trend that has been developing f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>ffc237e9</td>\n",
       "      <td>0</td>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>ffe1ca0d</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars have been around since the 1800's and hav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prompt_id                                               text  \\\n",
       "0     0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1     005db917          0  Transportation is a large necessity in most co...   \n",
       "2     008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3     00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4     00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "...        ...        ...                                                ...   \n",
       "1373  fe6ff9a5          1  There has been a fuss about the Elector Colleg...   \n",
       "1374  ff669174          0  Limiting car usage has many advantages. Such a...   \n",
       "1375  ffa247e0          0  There's a new trend that has been developing f...   \n",
       "1376  ffc237e9          0  As we all know cars are a big part of our soci...   \n",
       "1377  ffe1ca0d          0  Cars have been around since the 1800's and hav...   \n",
       "\n",
       "      generated  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "1373          0  \n",
       "1374          0  \n",
       "1375          0  \n",
       "1376          0  \n",
       "1377          0  \n",
       "\n",
       "[1378 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(notebook_config.DATA_DIR, \"train_essays.csv\"))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca02819-4443-44c9-8e14-3e775e1d2f52",
   "metadata": {},
   "source": [
    "Out of these we have been provided with only 3 generated essays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2d9454-fb82-4ffa-94e7-038190caca1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>82131f68</td>\n",
       "      <td>1</td>\n",
       "      <td>This essay will analyze, discuss and prove one...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>86fe4f18</td>\n",
       "      <td>1</td>\n",
       "      <td>I strongly believe that the Electoral College ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>eafb8a56</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car use causes pollution, increases c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prompt_id                                               text  \\\n",
       "704   82131f68          1  This essay will analyze, discuss and prove one...   \n",
       "740   86fe4f18          1  I strongly believe that the Electoral College ...   \n",
       "1262  eafb8a56          0  Limiting car use causes pollution, increases c...   \n",
       "\n",
       "      generated  \n",
       "704           1  \n",
       "740           1  \n",
       "1262          1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.generated == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfabc614-00e6-433b-bbf7-8e4c8eb13797",
   "metadata": {},
   "source": [
    "### Prompting\n",
    "\n",
    "Unless we procure more data the task is impossible. We will thus use prompting to produce our own generated Dataset.\n",
    "\n",
    "For this task we will use ChatGPT. The prompts were manually given using the official website, and exported the responses in Markdown form using the very convenient [ChatGPT Conv Down](https://addons.mozilla.org/en-US/firefox/addon/chatgpt-convdown/) Firefox addon. \n",
    "\n",
    "We used three prompting strategies to procure our generated Dataset:\n",
    "* The standard prompt (used for the human essays) without sources\n",
    "* The standard prompt using all the sources, with a role prompt to persuade the model into writing in a more simplistic tone mirroring the tone used in the human essays\n",
    "* The standard prompt with a role prompt, using a different subset of the sources each time \n",
    "* The standard prompt with a role prompt and a different subset of the human essays instead of sources, in order to \"mimic\" the tone and style of these essays\n",
    "\n",
    "This procedure was repeated for both types of prompts provided in the dataset (car-free cities and the Electoral College). The conversation context was periodically refreshed in order to avoid the model repeating its own generated essays. Following this procedure we hope to obtain a thorough dataset being as close as possible to the actual operational dataset.\n",
    "\n",
    "Details on the prompts themselves can be found in `report.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563372dd-10f9-4e2a-ada5-f891ee7fe282",
   "metadata": {},
   "source": [
    "### Dataset Caveats\n",
    "\n",
    "Unfortunately, even following the above strategy does not yield as good results as we would hope for. The biggest issue by far is that ChatGPT's output is formulaic and carries a distinct tone which is most of the time distinct from the students'. \n",
    "\n",
    "Role prompting and including human essays as input slightly alleviates the problem but does not eradicate it. Trying to explicitly set the tone of the generated essays leads to a complete collapse in tone, with outputs as *artistic* as the one below:\n",
    "\n",
    ">Dear [Senator's Name],\n",
    ">\n",
    ">Hope this letter finds you chill. I wanted to throw my two cents into the mix about the whole Electoral College debate. It's like a hot topic, right? Some say we should keep it, others are all about switching to the popular vote. Here's where I'm at.\n",
    ">\n",
    ">So, keeping the Electoral College â€“ it's got its quirks, but it's not all bad. I get it, voting for electors instead of the actual president seems a bit weird. But, like, it's been working, hasn't it? It's like a system our folks set up ages ago, and yeah, maybe it's not perfect, but it's got a rhythm.\" [...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a775b3-29f3-47f3-b6a1-50a1ac883649",
   "metadata": {},
   "source": [
    "### Importing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c9bda4-c928-4be4-8e53-5965949a6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def csv_output(df: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to a CSV file.\n",
    "\n",
    "    :param df: The DataFrame to be saved.\n",
    "    :type df: pd.DataFrame\n",
    "\n",
    "    :param filename: The name of the CSV file.\n",
    "    :type filename: str\n",
    "\n",
    "    :return: This function does not return anything.\n",
    "    :rtype: None\n",
    "    \"\"\"\n",
    "    file = os.path.join(notebook_config.OUTPUT_DIR, filename)\n",
    "    df.to_csv(file, encoding = 'utf8')\n",
    "    print(f\"File saved successfully as {file}\")\n",
    "\n",
    "\n",
    "def save_plot(filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves a plot to the output directory.\n",
    "\n",
    "    :param filename: The name of the file for the Figure.\n",
    "    :type filename: str\n",
    "    \"\"\"\n",
    "    path = os.path.join(notebook_config.OUTPUT_DIR, filename)\n",
    "    plt.savefig(path, bbox_inches=\"tight\")\n",
    "    print(f\"Figured saved to \" + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4af26a4e-793d-45f9-bab1-a5eac1ce2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gpt(file: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Read and extract ChatGPT responses from a file.\n",
    "\n",
    "    :param file: A string specifying the name of the file to be read.\n",
    "    :type file: str\n",
    "\n",
    "    :return: A pandas Series containing ChatGPT responses extracted from the file.\n",
    "    :rtype: pd.Series\n",
    "    \"\"\"\n",
    "    with open(os.path.join(notebook_config.DATA_DIR, file), encoding=\"utf8\") as f:\n",
    "        contents = f.read()\n",
    "    responses = list(filter(lambda x: x.startswith(\" ChatGPT\"), \n",
    "                        contents.split(\"##\")))\n",
    "    clear_responses = [res.replace(\"ChatGPT\", \"\").replace(\"~\", \"\").strip() \n",
    "                            for res in responses]\n",
    "    return pd.Series(clear_responses)\n",
    "\n",
    "\n",
    "def create_dataset(text_row: pd.Series, prompt_id: int, llm:str, source: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a dataset DataFrame from a pandas Series of text data.\n",
    "\n",
    "    :param text_row: A pandas Series containing text data.\n",
    "    :type text_row: pd.Series\n",
    "    :param prompt_id: An integer specifying the prompt ID for the dataset.\n",
    "    :type prompt_id: int\n",
    "    :param llm: A string specifying the language model used for generating the text.\n",
    "    :type llm: str\n",
    "    :param source: A string specifying the source of the text data.\n",
    "    :type source: str\n",
    "\n",
    "    :return: A DataFrame containing the created dataset.\n",
    "    :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    data_text = text_row\n",
    "    df = pd.DataFrame({\"id\": [str(hash(text)) for text in data_text],\n",
    "                     \"text\": data_text, \n",
    "                       \"prompt_id\": np.full_like(data_text, prompt_id),\n",
    "                       \"generated\": np.ones_like(data_text),\n",
    "                       \"llm\": llm,\n",
    "                       \"source\": source})\n",
    "    df.prompt_id = df.prompt_id.astype(int)\n",
    "    df.generated = df.generated.astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3d0b31-6109-4c08-9ba4-c2e00d3b26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "envir_df = create_dataset(read_gpt(\"chatgpt_cars.md\"), \n",
    "                          prompt_id=0, \n",
    "                          llm=\"ChatGPT\", \n",
    "                          source=\"Dimitris Tsirmpas\")\n",
    "elect_df = create_dataset(read_gpt(\"chatgpt_electoral.md\"),\n",
    "                          prompt_id=1, \n",
    "                          llm=\"ChatGPT\",\n",
    "                          source=\"Dimitris Tsirmpas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcbc4b-3b19-4f6e-a6f9-50d8f440d019",
   "metadata": {},
   "source": [
    "Additionally, we will procure generated essays from two other sources:\n",
    "* one dataset generated by Konstantina Liagkou for the purposes of the project\n",
    "* one dataset generated by Muhammad Rizqi, made publically available on his [recent post on LLM detection](https://medium.com/@mrizqi6061/how-i-generate-llm-generated-text-dataset-using-palm-google-generative-ai-in-google-colab-458c7797a5ac)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f59e75cb-1841-4c49-8ca8-7fea713bb823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>generated</th>\n",
       "      <th>llm</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4543371708701277945</td>\n",
       "      <td>Cars have been a major part of our lives for a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4100679533931189198</td>\n",
       "      <td>Limiting car usage has many advantages, such a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237461698100588951</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8931231566546949961</td>\n",
       "      <td>Cars are convenient, but they can be harmful t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2596327606650980419</td>\n",
       "      <td>Cars are a convenient way to get around, but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>3724606826730535089</td>\n",
       "      <td>**Limiting Car Usage**\\r\\n\\r\\nCars are a major...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-6831518509411687102</td>\n",
       "      <td>Limiting car usage has many advantages. For ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>-1684128838240839101</td>\n",
       "      <td>Limiting car usage is a great way to improve o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>2793838651774276948</td>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>3475724496411428946</td>\n",
       "      <td>Cars have been around for a long time and are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1375 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0     -4543371708701277945  Cars have been a major part of our lives for a...   \n",
       "1      4100679533931189198  Limiting car usage has many advantages, such a...   \n",
       "2      1237461698100588951  \"America's love affair with it's vehicles seem...   \n",
       "3     -8931231566546949961  Cars are convenient, but they can be harmful t...   \n",
       "4      2596327606650980419  Cars are a convenient way to get around, but t...   \n",
       "...                    ...                                                ...   \n",
       "1370   3724606826730535089  **Limiting Car Usage**\\r\\n\\r\\nCars are a major...   \n",
       "1371  -6831518509411687102  Limiting car usage has many advantages. For ex...   \n",
       "1372  -1684128838240839101  Limiting car usage is a great way to improve o...   \n",
       "1373   2793838651774276948  As we all know cars are a big part of our soci...   \n",
       "1374   3475724496411428946  Cars have been around for a long time and are ...   \n",
       "\n",
       "      prompt_id  generated   llm               source  \n",
       "0             0          1  PaLM  Konstantina Liagkou  \n",
       "1             0          1  PaLM  Konstantina Liagkou  \n",
       "2             0          1  PaLM  Konstantina Liagkou  \n",
       "3             0          1  PaLM  Konstantina Liagkou  \n",
       "4             0          1  PaLM  Konstantina Liagkou  \n",
       "...         ...        ...   ...                  ...  \n",
       "1370          0          1  PaLM  Konstantina Liagkou  \n",
       "1371          0          1  PaLM  Konstantina Liagkou  \n",
       "1372          0          1  PaLM  Konstantina Liagkou  \n",
       "1373          0          1  PaLM  Konstantina Liagkou  \n",
       "1374          0          1  PaLM  Konstantina Liagkou  \n",
       "\n",
       "[1375 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_car = pd.read_csv(os.path.join(notebook_config.DATA_DIR, \"cars_generated.csv\"))\n",
    "palm_car_df = create_dataset(palm_car.text, 0, \"PaLM\",  \"Konstantina Liagkou\")\n",
    "palm_car_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207b79f5-1ab7-49ea-8dba-552ba904edbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>generated</th>\n",
       "      <th>llm</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6738960532266317195</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9100200382113933130</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>818273850372787425</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8409859898402111579</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>9084953667816076133</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-6741392293829170114</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>-6033539590983051416</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>-6803680544958407954</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>-8240983046979722319</td>\n",
       "      <td>Dear Senator,\\r\\n\\r\\nI am writing to you today...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1375 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0      6738960532266317195  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "1      9100200382113933130  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "2             132621163553                                                NaN   \n",
       "3       818273850372787425  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "4      8409859898402111579  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "...                    ...                                                ...   \n",
       "1370   9084953667816076133  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "1371  -6741392293829170114  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "1372  -6033539590983051416  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "1373  -6803680544958407954  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "1374  -8240983046979722319  Dear Senator,\\r\\n\\r\\nI am writing to you today...   \n",
       "\n",
       "      prompt_id  generated   llm               source  \n",
       "0             1          1  PaLM  Konstantina Liagkou  \n",
       "1             1          1  PaLM  Konstantina Liagkou  \n",
       "2             1          1  PaLM  Konstantina Liagkou  \n",
       "3             1          1  PaLM  Konstantina Liagkou  \n",
       "4             1          1  PaLM  Konstantina Liagkou  \n",
       "...         ...        ...   ...                  ...  \n",
       "1370          1          1  PaLM  Konstantina Liagkou  \n",
       "1371          1          1  PaLM  Konstantina Liagkou  \n",
       "1372          1          1  PaLM  Konstantina Liagkou  \n",
       "1373          1          1  PaLM  Konstantina Liagkou  \n",
       "1374          1          1  PaLM  Konstantina Liagkou  \n",
       "\n",
       "[1375 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_elect = pd.read_csv(os.path.join(notebook_config.DATA_DIR, \"elections_generated.csv\"))\n",
    "palm_elect_df = create_dataset(palm_elect.text, 1, \"PaLM\",  \"Konstantina Liagkou\")\n",
    "palm_elect_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e1829-1732-4e44-be6c-bfc292735707",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/kingki19/llm-generated-essay-using-palm-from-google-gen-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d646fc-ce54-4033-b77a-a0de4e6ef276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>generated</th>\n",
       "      <th>llm</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7717755321899742660</td>\n",
       "      <td>## The Advantages of Limiting Car Usage\\n\\nIn ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7733363904607573248</td>\n",
       "      <td>The United States is a car-dependent nation, w...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3693064732800040085</td>\n",
       "      <td>In recent years, there has been a growing move...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6342195701252945583</td>\n",
       "      <td>In recent years, there has been a growing move...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3771873485489518116</td>\n",
       "      <td>In the past few decades, the United States has...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>273763551878372001</td>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>1358268637434280974</td>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>1495853012532520985</td>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>-6763873040510148860</td>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>6404013787904331045</td>\n",
       "      <td>Dear Senator,\\n\\nI am writing to you today to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Muhammad Rizqi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1384 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0     -7717755321899742660  ## The Advantages of Limiting Car Usage\\n\\nIn ...   \n",
       "1      7733363904607573248  The United States is a car-dependent nation, w...   \n",
       "2      3693064732800040085  In recent years, there has been a growing move...   \n",
       "3     -6342195701252945583  In recent years, there has been a growing move...   \n",
       "4      3771873485489518116  In the past few decades, the United States has...   \n",
       "...                    ...                                                ...   \n",
       "1379    273763551878372001  Dear Senator,\\n\\nI am writing to you today to ...   \n",
       "1380   1358268637434280974  Dear Senator,\\n\\nI am writing to you today to ...   \n",
       "1381   1495853012532520985  Dear Senator,\\n\\nI am writing to you today to ...   \n",
       "1382  -6763873040510148860  Dear Senator,\\n\\nI am writing to you today to ...   \n",
       "1383   6404013787904331045  Dear Senator,\\n\\nI am writing to you today to ...   \n",
       "\n",
       "      prompt_id  generated   llm          source  \n",
       "0             0          1  PaLM  Muhammad Rizqi  \n",
       "1             0          1  PaLM  Muhammad Rizqi  \n",
       "2             0          1  PaLM  Muhammad Rizqi  \n",
       "3             0          1  PaLM  Muhammad Rizqi  \n",
       "4             0          1  PaLM  Muhammad Rizqi  \n",
       "...         ...        ...   ...             ...  \n",
       "1379          1          1  PaLM  Muhammad Rizqi  \n",
       "1380          1          1  PaLM  Muhammad Rizqi  \n",
       "1381          1          1  PaLM  Muhammad Rizqi  \n",
       "1382          1          1  PaLM  Muhammad Rizqi  \n",
       "1383          1          1  PaLM  Muhammad Rizqi  \n",
       "\n",
       "[1384 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_general = pd.read_csv(os.path.join(notebook_config.DATA_DIR, \"LLM_generated_essay_PaLM.csv\"))\n",
    "palm_general_df = create_dataset(palm_general.text, -1, \"PaLM\", \"Muhammad Rizqi\")\n",
    "palm_general_df.prompt_id = palm_general.prompt_id.astype(int)\n",
    "palm_general_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb67fe1f-9041-40cf-9084-86d0effdc8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>llm</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>fe6ff9a5</td>\n",
       "      <td>1</td>\n",
       "      <td>There has been a fuss about the Elector Colleg...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>ff669174</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage has many advantages. Such a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>ffa247e0</td>\n",
       "      <td>0</td>\n",
       "      <td>There's a new trend that has been developing f...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>ffc237e9</td>\n",
       "      <td>0</td>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>ffe1ca0d</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars have been around since the 1800's and hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prompt_id                                               text  \\\n",
       "0     0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1     005db917          0  Transportation is a large necessity in most co...   \n",
       "2     008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3     00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4     00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "...        ...        ...                                                ...   \n",
       "1373  fe6ff9a5          1  There has been a fuss about the Elector Colleg...   \n",
       "1374  ff669174          0  Limiting car usage has many advantages. Such a...   \n",
       "1375  ffa247e0          0  There's a new trend that has been developing f...   \n",
       "1376  ffc237e9          0  As we all know cars are a big part of our soci...   \n",
       "1377  ffe1ca0d          0  Cars have been around since the 1800's and hav...   \n",
       "\n",
       "      generated    llm       source  \n",
       "0             0  Human  Competition  \n",
       "1             0  Human  Competition  \n",
       "2             0  Human  Competition  \n",
       "3             0  Human  Competition  \n",
       "4             0  Human  Competition  \n",
       "...         ...    ...          ...  \n",
       "1373          0  Human  Competition  \n",
       "1374          0  Human  Competition  \n",
       "1375          0  Human  Competition  \n",
       "1376          0  Human  Competition  \n",
       "1377          0  Human  Competition  \n",
       "\n",
       "[1378 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_df = train_df\n",
    "comp_df[\"llm\"] = np.where(comp_df.generated==1, \"Unknown\", \"Human\")\n",
    "comp_df[\"source\"] = \"Competition\"\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56063d4b-7831-4b63-965f-60250e80369c",
   "metadata": {},
   "source": [
    "And we will merge all the datasets into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b79b8d3-d8ca-49c5-9cea-b46c4d5fa8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>generated</th>\n",
       "      <th>llm</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4543371708701277945</td>\n",
       "      <td>Cars have been a major part of our lives for a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4100679533931189198</td>\n",
       "      <td>Limiting car usage has many advantages, such a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237461698100588951</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8931231566546949961</td>\n",
       "      <td>Cars are convenient, but they can be harmful t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2596327606650980419</td>\n",
       "      <td>Cars are a convenient way to get around, but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5766</th>\n",
       "      <td>fe6ff9a5</td>\n",
       "      <td>There has been a fuss about the Elector Colleg...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767</th>\n",
       "      <td>ff669174</td>\n",
       "      <td>Limiting car usage has many advantages. Such a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5768</th>\n",
       "      <td>ffa247e0</td>\n",
       "      <td>There's a new trend that has been developing f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5769</th>\n",
       "      <td>ffc237e9</td>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5770</th>\n",
       "      <td>ffe1ca0d</td>\n",
       "      <td>Cars have been around since the 1800's and hav...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Human</td>\n",
       "      <td>Competition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5771 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0     -4543371708701277945  Cars have been a major part of our lives for a...   \n",
       "1      4100679533931189198  Limiting car usage has many advantages, such a...   \n",
       "2      1237461698100588951  \"America's love affair with it's vehicles seem...   \n",
       "3     -8931231566546949961  Cars are convenient, but they can be harmful t...   \n",
       "4      2596327606650980419  Cars are a convenient way to get around, but t...   \n",
       "...                    ...                                                ...   \n",
       "5766              fe6ff9a5  There has been a fuss about the Elector Colleg...   \n",
       "5767              ff669174  Limiting car usage has many advantages. Such a...   \n",
       "5768              ffa247e0  There's a new trend that has been developing f...   \n",
       "5769              ffc237e9  As we all know cars are a big part of our soci...   \n",
       "5770              ffe1ca0d  Cars have been around since the 1800's and hav...   \n",
       "\n",
       "      prompt_id  generated    llm               source  \n",
       "0             0          1   PaLM  Konstantina Liagkou  \n",
       "1             0          1   PaLM  Konstantina Liagkou  \n",
       "2             0          1   PaLM  Konstantina Liagkou  \n",
       "3             0          1   PaLM  Konstantina Liagkou  \n",
       "4             0          1   PaLM  Konstantina Liagkou  \n",
       "...         ...        ...    ...                  ...  \n",
       "5766          1          0  Human          Competition  \n",
       "5767          0          0  Human          Competition  \n",
       "5768          0          0  Human          Competition  \n",
       "5769          0          0  Human          Competition  \n",
       "5770          0          0  Human          Competition  \n",
       "\n",
       "[5771 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df = pd.concat([palm_car_df, \n",
    "                         palm_elect_df, \n",
    "                         palm_general_df,\n",
    "                         envir_df, \n",
    "                         elect_df, \n",
    "                         comp_df], ignore_index=True)\n",
    "original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476b1f6-656d-45f5-b447-054bea6ca8b3",
   "metadata": {},
   "source": [
    "Before we begin we need to perform some data clearing steps, such as removing null values, empty texts or unrecognized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5c2ee71-f405-4608-9980-87408af26f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.text = original_df.text.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b59cc8-1594-4829-91fe-2101a67eaee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>generated</th>\n",
       "      <th>llm</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>132621163553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PaLM</td>\n",
       "      <td>Konstantina Liagkou</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id text  prompt_id  generated   llm               source\n",
       "8     132621163553  NaN          0          1  PaLM  Konstantina Liagkou\n",
       "10    132621163553  NaN          0          1  PaLM  Konstantina Liagkou\n",
       "26    132621163553  NaN          0          1  PaLM  Konstantina Liagkou\n",
       "28    132621163553  NaN          0          1  PaLM  Konstantina Liagkou\n",
       "30    132621163553  NaN          0          1  PaLM  Konstantina Liagkou\n",
       "...            ...  ...        ...        ...   ...                  ...\n",
       "2723  132621163553  NaN          1          1  PaLM  Konstantina Liagkou\n",
       "2725  132621163553  NaN          1          1  PaLM  Konstantina Liagkou\n",
       "2732  132621163553  NaN          1          1  PaLM  Konstantina Liagkou\n",
       "2736  132621163553  NaN          1          1  PaLM  Konstantina Liagkou\n",
       "2739  132621163553  NaN          1          1  PaLM  Konstantina Liagkou\n",
       "\n",
       "[568 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_str(x: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detects non-strings without relying on type signature.\n",
    "    \"\"\"\n",
    "    # this is awful, I hate it but pandas has forced my hand\n",
    "    try:\n",
    "        x.strip()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "mask = ~ original_df.text.apply(try_str)\n",
    "original_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1537c04-0237-4c18-b6d6-d71b17103afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = original_df[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874ff29-7b45-4d50-bb8e-bc7804edb9bd",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Unlike the previous clearing steps, in this section we define the following preprocessing steps which will be performed only on the training dataset:\n",
    "* Remove all placeholder tags such as [Your Name], and which are exclusively used by ChatGPT\n",
    "* Turn all text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b51b2db0-0881-49fd-8a3b-f512f9b941ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full training dataset\n",
    "df = original_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95fd8f8c-177a-48ef-9e20-7db15245b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocess(text: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Preprocess a pandas Series of text data by removing brackets and converting to lowercase.\n",
    "\n",
    "    :param text: A pandas Series containing text data.\n",
    "    :type text: pd.Series\n",
    "\n",
    "    :return: The preprocessed pandas Series.\n",
    "    :rtype: pd.Series\n",
    "    \"\"\"\n",
    "    bracket_regex = r\"\\[.*?\\]\"\n",
    "    text = text.apply(lambda x: re.sub(bracket_regex, \"\", x))\n",
    "    \n",
    "    text = text.apply(lambda x: x.lower())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d918e15-163a-434b-9d69-063564e61913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = preprocess(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780d038-34f9-4f95-b6c2-3e87a8efd3a0",
   "metadata": {},
   "source": [
    "The resulting text resembles the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d126d4c0-2dba-49fd-b367-8984362f9a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generated essays in the dataset: 3828\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of generated essays in the dataset: {(df.generated == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f3514aa-5559-4436-9993-185b1116bbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf101a9-53f9-4ee4-b65c-6f77f5c15f09",
   "metadata": {},
   "source": [
    "We also may need to convert the dataframe's types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec3a81-9076-454a-a436-a9f3f2f603d3",
   "metadata": {},
   "source": [
    "### More augmentation?\n",
    "\n",
    "Another idea would be to use other standard NLP augmentation techniques such as randomly deleting/inserting words, purposefully inserting typographical mistakes in the text or even [replacing words with close synonyms](https://www.kaggle.com/code/rohitsingh9990/data-augmentation-by-synonym-replacement). However, all of these strategies would ultimately hurt the quality of our dataset, since typos and specific words are very valuable tools in LLM detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124c4a3e-9ced-4036-a0b9-b2ad60fb2f74",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "In this section we train multiple classifiers on our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238e13b-7faf-4c00-b67c-d5a06308598a",
   "metadata": {},
   "source": [
    "Before we begin we must select an appropriate text representation, since our classifiers only work with numerical vectors, not strings. The two implementations that we use are:\n",
    "* TF-IDF vectors\n",
    "* Word2Vec embeddings\n",
    "\n",
    "Both approaches have upsides and downsides. TF-IDF vectors are adept at automatically selecting features by weighting, assigning more importance to rare words. In fact, TF-IDF can be applied to any combination of n-grams, thus preserving some information about the ordering and context of words, with more computational cost. \n",
    "\n",
    "We will be using a 3-gram, 4-gram and 5-gram TF-IDF schema, since it is proven competitive when compared to standard embeddings `[1]` (although in this paper the task wasn't strictly *document* classification), without bearing too much computational cost on our classifiers.  \n",
    "\n",
    "Word2Vec vectors on the other hand are dense document embeddings which take into account the semantic meaning of the words and not just their frequency. The downside is their fixed size (300x1 embedding for any word), which means that we either need to concatenate the vectors of each word in the text, or use a merging scheme in order to produce a document embedding from the individual word embeddings. \n",
    "\n",
    "In this project, we use the default Word2Vec behavior, which averages the word embeddings. While powerful, this technique may lead to significant data loss and overfitting if we are not sure the document embeddings of the train set are similar to the operational set (the data our algorithms will be run on and which aren't included in the train and test sets).\n",
    "\n",
    "> [1] [TF-IDF Character N-grams versus Word Embedding-based Models for Fine-grained Event Classification: A Preliminary Study](https://aclanthology.org/2020.aespen-1.6) (Piskorski & Jacquet, AESPEN 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053324eb-b968-4c23-9934-eac1893ece8e",
   "metadata": {},
   "source": [
    "The goal is to use the simplest model possible that has an acceptably efficient performance. We start with discriminant classifiers such as **Naive Bayes** and **Logistic Regression** and consider Ensemble learning classifiers such as **Random Forest**, **Extra-Random Forest** and **Adaboost**. The results guide us on whether we would need to use more powerful NLP models such as RNNs, LSTMs or even (pretrained) Transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f0e7fc6-5c7d-4689-9fae-f50be4ff655a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Word2Vec model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading Word2Vec model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73460c28-50ce-4a7c-8092-9f3ad8c7e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18ffbf15-eb47-41f0-8c56-b9c5d5b32390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Model downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9246e340-ac9c-47f2-aca3-99b17c59f62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "print(\"Loading Word2Vec model...\")\n",
    "word2vec = spacy.load('en_core_web_md')\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa7372c-5ef2-42a4-b129-3f933a004cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670a5f9f22564fa9b9c3568400062c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b322c1e5fb45f19f05c583fc93e3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 52\u001b[0m\n\u001b[0;32m     48\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(word2vec(text)\u001b[38;5;241m.\u001b[39mvector)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n\u001b[1;32m---> 52\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compute_embeddings(df\u001b[38;5;241m.\u001b[39mtext, rem_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[24], line 48\u001b[0m, in \u001b[0;36mcompute_embeddings\u001b[1;34m(text, rem_stopwords)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(text):\n\u001b[1;32m---> 48\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(word2vec(text)\u001b[38;5;241m.\u001b[39mvector)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:264\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:285\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\spacy\\ml\\tb_framework.py:34\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[1;32m---> 34\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m ParserStepModel(\n\u001b[0;32m     35\u001b[0m         X,\n\u001b[0;32m     36\u001b[0m         model\u001b[38;5;241m.\u001b[39mlayers,\n\u001b[0;32m     37\u001b[0m         unseen_classes\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munseen_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     38\u001b[0m         train\u001b[38;5;241m=\u001b[39mis_train,\n\u001b[0;32m     39\u001b[0m         has_upper\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_upper\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\spacy\\ml\\parser_model.pyx:250\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[1;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\concatenate.py:57\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m---> 57\u001b[0m     Ys, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers])\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Ys[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     59\u001b[0m         data_l, backprop \u001b[38;5;241m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\concatenate.py:57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m---> 57\u001b[0m     Ys, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers])\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Ys[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     59\u001b[0m         data_l, backprop \u001b[38;5;241m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\with_array.py:36\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     33\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\with_array.py:91\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ragged_forward\u001b[39m(\n\u001b[0;32m     88\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     90\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 91\u001b[0m     Y, get_dX \u001b[38;5;241m=\u001b[39m layer(Xr\u001b[38;5;241m.\u001b[39mdataXd, is_train)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYr: Ragged) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ragged:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[38;5;241m.\u001b[39mdataXd), dYr\u001b[38;5;241m.\u001b[39mlengths)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\concatenate.py:57\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m---> 57\u001b[0m     Ys, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers])\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Ys[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     59\u001b[0m         data_l, backprop \u001b[38;5;241m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\concatenate.py:57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m---> 57\u001b[0m     Ys, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers])\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Ys[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     59\u001b[0m         data_l, backprop \u001b[38;5;241m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\manis\\Lib\\site-packages\\thinc\\layers\\hashembed.py:72\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, ids, is_train)\u001b[0m\n\u001b[0;32m     70\u001b[0m seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     71\u001b[0m keys \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mhash(ids, seed) \u001b[38;5;241m%\u001b[39m nV\n\u001b[1;32m---> 72\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mgather_add(vectors, keys)\n\u001b[0;32m     73\u001b[0m drop_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_train:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def remove_stopwords(words: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove stopwords from a given string of words.\n",
    "\n",
    "    :param words: A string containing words.\n",
    "    :type words: str\n",
    "\n",
    "    :return: A string with stopwords removed.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    word_tokens = word_tokenize(words)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "     \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return \" \".join(filtered_sentence)\n",
    "    \n",
    "\n",
    "def compute_embeddings(text: pd.Series, rem_stopwords: bool=True) -> list[str]:\n",
    "    \"\"\"\n",
    "    Compute word embeddings for a pandas Series of text data.\n",
    "\n",
    "    :param text: A pandas Series containing text data.\n",
    "    :type text: pd.Series\n",
    "    :param rem_stopwords: A boolean indicating whether to remove stopwords from the text.\n",
    "                          Default is True.\n",
    "    :type rem_stopwords: bool\n",
    "\n",
    "    :return: A list of word embeddings.\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    if remove_stopwords:\n",
    "        print(\"Removing stopwords...\")\n",
    "        text = text.progress_apply(remove_stopwords)\n",
    "    \n",
    "    print(\"Computing embeddings...\")\n",
    "    for text in tqdm(text):\n",
    "        embeddings.append(word2vec(text).vector)\n",
    "        \n",
    "    return embeddings\n",
    "\n",
    "df[\"embedding\"] = compute_embeddings(df.text, rem_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c7db5-f262-4d56-a997-66aafc8e7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def get_data_from_text(vectorizer: TfidfVectorizer, \n",
    "                       data_train: pd.DataFrame, \n",
    "                       data_test: pd.DataFrame) -> dict[str, np.ndarray]:\n",
    "    vectorizer = vectorizer.fit(data_train.text)\n",
    "    x_train = vectorizer.transform(data_train.text)\n",
    "    x_test = vectorizer.transform(data_test.text)\n",
    "    \n",
    "    embed_x_train = np.array([x for x in data_train.embedding])\n",
    "    embed_x_test = np.array([x for x in data_test.embedding])\n",
    "    \n",
    "    y_train = data_train.generated.values\n",
    "    y_test = data_test.generated.values\n",
    "\n",
    "    return {\"x_train\": x_train, \n",
    "            \"y_train\": y_train,\n",
    "            \"x_test\": x_test, \n",
    "            \"y_test\": y_test, \n",
    "            \"embed_x_train\": embed_x_train, \n",
    "            \"embed_x_test\": embed_x_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2112495-7c53-4fe1-a52b-2fe8adc35502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_train, data_test = train_test_split(df,\n",
    "                                         train_size=0.7, \n",
    "                                         test_size=0.3, \n",
    "                                         stratify=df.generated,\n",
    "                                         random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents=\"unicode\",\n",
    "                             ngram_range=(3,5), \n",
    "                             max_df=0.9, \n",
    "                             min_df=0.05)\n",
    "data = get_data_from_text(vectorizer, data_train, data_test)\n",
    "\n",
    "x_train = data[\"x_train\"]\n",
    "x_test = data[\"x_test\"]\n",
    "\n",
    "embed_x_train = data[\"embed_x_train\"]\n",
    "embed_x_test = data[\"embed_x_test\"]\n",
    "\n",
    "y_train = data[\"y_train\"]\n",
    "y_test = data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470494ba-7eb3-496d-9d83-d8b83a94d59b",
   "metadata": {},
   "source": [
    "The metric we will be using is Macro-F1 average.\n",
    "\n",
    "- **F1** is a metric used to balance the need for making sure our classifications for a category are both correct (precision) and represent as many of the actual cases of the category as possible (recall).\n",
    "- **Macro-F1** is the unweighted average of all F1 metrics for each class. We choose Macro F1 instead of a weighted average because we have an unbalanced dataset (Generated essays data are a small fraction of overall essays)\n",
    "\n",
    "\n",
    "Thus, we want to use a metric which favors both thorough and precise classifiers, and which also assigns equal importance to our smaller classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502c8d8-64f1-42d5-8885-74e4d7dccae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def cross_val_res(model, x, y, scoring=None, cv=5):\n",
    "    \"\"\"\n",
    "    Minor utility method, wraping cross_val_score.\n",
    "    \"\"\"\n",
    "    if scoring is None:\n",
    "        scoring = \"f1_macro\"\n",
    "    res = cross_val_score(model, x, y, cv=cv, scoring=scoring)\n",
    "    return res.mean(), res.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916b8b8-a7f5-4913-ab72-08bccaeee0f8",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28556888-ed28-4fa2-ab31-4408f5edabcd",
   "metadata": {},
   "source": [
    "We will first run a \"fake\" classifier which only guesses the majority category.\n",
    "\n",
    "This dummy model thus completely disregards the input features and serves as a useful baseline with which to compare the subsequent classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8d6ca-c3af-4e0d-a808-60b5515bcb12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "\n",
    "majority_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "res = cross_val_res(majority_model, x_train, y_train)\n",
    "print(f\"Dummy Classifier mean macro F1-score {res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7735259-ac2f-44ef-abb2-edfb09d296de",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f69629-6178-4b26-9703-630c507babe7",
   "metadata": {},
   "source": [
    "Naive Bayes is a very cheap and easy-to-interpret classifier, which checks for the probability that each individual word in the text will belong in any language. We generally want to use the simplest model for the job, and so we start with this reliable model which has proven itself in many fields in the past.\n",
    "\n",
    "The `sklearn` library gives us access to many variations of Naive Bayes, each specialized in its own field. For this NLP task, we will be using `MultinomialNB`, which was suggested by [this blogpost](https://towardsdatascience.com/naive-bayes-classifiers-for-text-classification-be0d133d35ba).\n",
    "\n",
    "Also note that Naive Bayes cannot be used on dense non-strictly-positive arrays, such as Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403adf6-3e64-4431-ac50-8f68df3a14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# naive bayes needs dense arrays to work \n",
    "naive_x_train = x_train.toarray()\n",
    "naive_x_test = x_test.toarray()\n",
    "\n",
    "naive_model = MultinomialNB()\n",
    "res = cross_val_res(naive_model, x_train, y_train)\n",
    "print(f\"Naive Bayes  with TF-IDF vectors Macro-F1 \\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d7e4a0-2623-46e1-9cbc-6ae28bb87568",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c15dbe-a7c6-40e5-a18e-ef52822c1b01",
   "metadata": {},
   "source": [
    "LogisticRegression despite its name is a linear classifier, meaning that it attempts to linearly separate the data into distinct categories. This interpretation does not apply well to a NLP task, but means that the classifier retains some very useful properties:\n",
    "\n",
    "- The solution we get is a global optimum, meaning that it's the best we can get with the provided data. This means no hyper-parameter tuning is necessary and we can use the classifier as-is.\n",
    "- It's a simple and very easy to compute classifier, since it solves a (mathematically simple) linear problem, albeit with some restrictions (technically those restrictions force it to use gradient descent, but the calculations are much easier than say, a neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50250bf6-0adc-4bfe-b334-6374c29ebbb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore warnings about deprecated methods in libraries\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    res = cross_val_res(lr, x_train, y_train)\n",
    "    print(f\"Logistic Regression with TF-IDF vectors Macro-F1\\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507dd90-73fb-46ab-a19f-fe3dbf6cb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    # ignore warnings about deprecated methods in libraries\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    res = cross_val_res(lr, embed_x_train, y_train)\n",
    "    print(f\"Logistic Regression with Word2Vec Embeddings Macro-F1\\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ccc10-19c4-482e-b506-47b85a187d63",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c9d54-b472-4b37-8068-1d4579acadee",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble algorithm, which means it uses many simpler algorithms which then \"vote\" on a final decision. It has proven to be a good classifier on complex tasks, it combats overfitting by design (essentially by utilizing random chance in its training phase) and is still fairly easy to interpret.\n",
    "\n",
    "The drawback is first and foremost computational, since we need to train many smaller classifiers, which may by themselves be computationally expensive (this is somewhat offset by the fact that the classifiers are indepednent and can be computed in parallel). Additionally, Random Forest is a non-parametric method which means that it is generally memory-intensive and may be slow to run on operational data. Finally, we also need to tune hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242df209-18a9-4652-9863-31a8560a1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "forest_model = RandomForestClassifier(n_estimators=500,\n",
    "                                      criterion=\"entropy\", \n",
    "                                      n_jobs=-1)\n",
    "res = cross_val_res(forest_model, x_train, y_train)\n",
    "print(f\"Random Forest with TF-IDF vectors Macro-F1\\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f485f2f-e957-41a2-84dc-5f3fbb92ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_res(forest_model, embed_x_train, y_train)\n",
    "print(f\"Random Forest with Word2Vec Embeddings Macro-F1\\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb5296-f38c-48dc-8e61-afc90cf31287",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier\n",
    "\n",
    "This classifier is a variant of Random Forest which assigns rules utilizing random chance when deciding rules, in order to reduce overfitting at the cost of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4ed15-3743-4d0a-8ddd-663127396ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "extra_model = ExtraTreesClassifier(n_estimators=500, \n",
    "                                     max_depth=None,\n",
    "                                     min_samples_split=0.05, \n",
    "                                   n_jobs=-1)\n",
    "res = cross_val_res(extra_model, x_train, y_train)\n",
    "print(f\"Extra Trees Forest with TF-IDF vectors Macro-F1\\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b07ffb-9ffe-47e2-b8cd-3e2d2893f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_res(extra_model, embed_x_train, y_train)\n",
    "print(f\"Extra Trees Forest with Word2Vec Embeddings Macro-F1\\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f4ffe4-87f9-4179-9f43-c2641b26f015",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27a788-11b2-4509-8218-8e906f694a67",
   "metadata": {},
   "source": [
    "Adaboost is the logical conclusion of Random Forests, where each voter considers a very specific \"rule\" that needs to be followed. The next voter then considers the most important rule to distinguish between the categories for all the clases that the first could not reliably classify, and so on.\n",
    "\n",
    "This classifier is generally more compact and competent than a simple Random Forest, but is more computationally expensive during training because we cannot train it in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb17ba-62b2-4ae1-a27c-c0793176747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "ada_model = AdaBoostClassifier(n_estimators=200)\n",
    "res = cross_val_res(ada_model, x_train, y_train)\n",
    "print(f\"AdaBoost with TF-IDF vectors Macro-F1:\\n mean {res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3acf5-9beb-48b6-a43f-36c4ff370d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = cross_val_res(ada_model, embed_x_train, y_train)\n",
    "print(f\"AdaBoost with Word2Vec Embeddings Macro-F1:\\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a2067-f89a-42ed-b044-6a3cc981c83f",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194f1e38-bc98-458f-a357-a0dfe89a4695",
   "metadata": {},
   "source": [
    "From the scores above it's clear that the Adaboost classifier with TF-IDF embeddings outperforms the other classifiers. The model shows a (utopic?) macro-f1 training score of 1, which usually means it has overfitted the training set. However the test set also displays an almost identical Macro-F1 score.\n",
    "\n",
    "These results may be explained by how dissimilar our two classes are, which we already suspect is the case. Human essays feature typos and more casual and simple language, while LLM generated essays are formulaic, feature the same structure and often even the same formal vocabulary. This hypothesis will be tested in the next Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f32b0-693a-4928-b8ed-870f5c25b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = forest_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9b628-55be-4664-be08-f27d83f1cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = [{\"n_estimators\": np.linspace(50, 2000, 10).astype(int)}]\n",
    "search = GridSearchCV(estimator=best_model, param_grid=param_grid, cv=4)\n",
    "search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9920f5-1d64-4fe4-ad7c-f7c2eaafa9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search.best_estimator_\n",
    "best_pred = best_model.predict(x_test)\n",
    "print(classification_report(y_test, best_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09a2d5-0075-44e5-809b-f9d201b16452",
   "metadata": {},
   "source": [
    "### Interpreting the best model\n",
    "\n",
    "In order to intrepret the decision making of our model we can use the `LimeTextExplainer` module which can show us a breakdown of the $n$ most important words which led to the classification of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b1c65-f16e-4c08-93c4-5eadae3e86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "\n",
    "best_pred = best_model.predict(x_test)\n",
    "c = make_pipeline(vectorizer, best_model)\n",
    "explainer = LimeTextExplainer(class_names=best_model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52309649-496d-4fef-b926-ad7b800b5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# code for lime text and graph attributed to Ioannis Pavlopoulos\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "fig.set_size_inches(10, 4)\n",
    "\n",
    "# explanation for (correct) human classification\n",
    "example_human = data_test[(data_test.generated == 0) & (best_pred == 0)].head(256)\n",
    "exp_human = explainer.explain_instance(example_human.text.iloc[0] , c.predict_proba)\n",
    "exp_human_reversed = [(name, -value) for name, value in exp_human.as_list()]\n",
    "word_scores_human = pd.DataFrame(exp_human.as_list(), columns=[\"word\", \"xscore\"]) \n",
    "word_scores_human['color'] = word_scores_human.xscore.apply(lambda x: 'g' if x>0 else 'r')\n",
    "\n",
    "axes[0].barh(word_scores_human.word, word_scores_human.xscore, color=word_scores_human.color)\n",
    "axes[0].set_title(\"Human essay\")\n",
    "\n",
    "# explanation for generated classification\n",
    "example = data_test[(data_test.generated == 1) & (best_pred == 1)].head(256)\n",
    "exp = explainer.explain_instance(example.text.iloc[0] , c.predict_proba)\n",
    "word_scores = pd.DataFrame(exp.as_list(), columns=[\"word\", \"xscore\"])\n",
    "word_scores['color'] = word_scores.xscore.apply(lambda x: 'g' if x>0 else 'r')\n",
    "\n",
    "axes[1].barh(word_scores.word, word_scores.xscore, color=word_scores.color)\n",
    "axes[1].set_title(\"Generated essay\")\n",
    "\n",
    "fig.suptitle(\"Classification attribution per word\")\n",
    "fig.text(0.5, -0.05, 'Attribution (left/red: human, right/green generated)', ha='center')\n",
    "fig.tight_layout(h_pad=3, w_pad=3) #believe it or not this *is* different from pad=3\n",
    "\n",
    "save_plot(\"attribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d2a25-60da-4f92-ac82-6d5810e57995",
   "metadata": {},
   "source": [
    "The results aren't very interpretable since we:\n",
    "* Did not remove stopwords from the texts for the TF-IDF representation\n",
    "* Use an n-gram model instead of a simple uni-gram model.\n",
    "\n",
    "What we can gather from this graph however, is that our model may be overfitting on the current data. Our intuition would claim that simpler words, more frequently seen in the student essays, would be included in the human classification attribution compared to the generated attribution graph. Additionally, the simple fact that stopwords are included among the most important features is a good indication of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4949c4-b794-41a1-be98-c390e2159c47",
   "metadata": {},
   "source": [
    "## Analyzing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e0201-c70b-4fc9-aa42-79c75fa87d86",
   "metadata": {},
   "source": [
    "In this section we investigate the probable overfitting issue by analyzing our generated data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b7d3d-b5d7-4ea2-9a0c-d64b6719da05",
   "metadata": {},
   "source": [
    "### Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff2564-317d-4331-aa16-c0558443f42d",
   "metadata": {},
   "source": [
    "The first step towards interpreting the results of our classifiers is to gauge the characteristics of our dataset. We can estimate how easily distinguishable the two classes (generated / human) are by calculating the similarity between each generated post compared to all human ones.\n",
    "\n",
    "There are many similarity scores for text data, each with its own criteria, strength and weaknesses. Two of the most prominent ones are:\n",
    "* Cosine Similarity, usually used for text embeddings\n",
    "* Jaccard Similarity, which is applied to the raw text, and calculates the ratio of common to total words between the texts. This score is biased towards large texts and does not take into account multiple instances of a word.\n",
    "\n",
    "For the purposes of our analysis we use the Jaccard Similarity, since the TF-IDF representation that we have chosen inherently uses the existence of words instead of their semantic meaning. Thus, the Jaccard Similarity score should be directly related to the ability of our classifiers to distinguish the classes in our dataset.\n",
    "\n",
    "We will be computing the mean and max similarity for every generated text compared to all human texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e94be-677b-452e-afc1-b7735d14e40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df = df[df.generated == 1]\n",
    "human_df = df[df.generated == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bac36d-fef1-4f93-a326-a6755d7e79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1: set[str], set2: set[str]) -> float:\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "mean_similarity = []\n",
    "max_similarity = []\n",
    "\n",
    "print(\"Calculating similarity scores...\")\n",
    "for x in tqdm(generated_df.text):\n",
    "    similarities = []\n",
    "    for y in human_df.text:\n",
    "      similarities.append(jaccard_similarity(set(x.split()), set(y.split())))\n",
    "                                        \n",
    "    mean_similarity.append(np.mean(similarities))\n",
    "    max_similarity.append(np.max(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eafdda-9be0-4f89-9f60-a0941a83c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame({\"id\": generated_df.id,\n",
    "                              \"text\": generated_df.text,\n",
    "                              \"mean\": mean_similarity,\n",
    "                              \"max\": max_similarity})\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079160c-a28f-40ba-9eab-c48370de8739",
   "metadata": {},
   "source": [
    "And plot them, to get a better picture of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085c8bb-b704-4c2b-9706-dd71ccb7431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import textwrap\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle(\"Similarity between generated and human essays\")\n",
    "fig.tight_layout(pad=2.0)\n",
    "\n",
    "sns.histplot(similarity_df, palette={\"mean\": \"blue\", \"max\": \"green\"}, ax=ax1)\n",
    "\n",
    "ax1.set_xlim(0, 0.4)\n",
    "ax1.set_xlabel(\"Jaccard Similarity\")\n",
    "ax1.set_ylabel(\"Number of Genereated Essays\")\n",
    "ax1.set_title(\"Main data\")\n",
    "\n",
    "\n",
    "sns.histplot(similarity_df, palette={\"mean\": \"blue\", \"max\": \"green\"}, ax=ax2)\n",
    "ax2.set_xlabel(\"Jaccard Similarity\")\n",
    "ax2.set_ylabel(\"Number of Genereated Essays\")\n",
    "ax2.set_xlim(0.4)\n",
    "ax2.set_ylim(0, 30)\n",
    "ax2.set_title(\"Outliers\")\n",
    "\n",
    "save_plot(\"similarity.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aac8c0-6775-46c5-9675-8973b8a81621",
   "metadata": {},
   "source": [
    "The aggregated similarities are not normally distributed (significantly \"fat\" tails). Most generated texts are not at all similar with the human essays, although we notice some being very similar with at least one human text and more than 20 being completely identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa6fde-7a28-4a11-a15d-d3a74675c95d",
   "metadata": {},
   "source": [
    "Another important pattern would be if mean and max similarity are correlated. By plotting them against each other we can see a clear linear correlation between them, with many significant outliers for $mean > 0.12$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2d7c2-9e1c-49e3-979f-fd17728bab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"mean\", y=\"max\", data=similarity_df)\n",
    "\n",
    "plt.title(\"Mean vs Max similarity between generated and human essays\")\n",
    "plt.xlabel(\"Mean Jaccard Similarity\")\n",
    "plt.ylabel(\"Max Jaccard Similarity\")\n",
    "\n",
    "save_plot(\"similarity_mean_max.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1723101-d941-4d05-bf35-a3e5ae23718f",
   "metadata": {},
   "source": [
    "We can look at some of the outliers sorted by their max similarity with all human texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78455aa2-bb50-4f09-8ee7-5b188e12598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = df.merge(similarity_df[similarity_df[\"max\"] > 0.9], how=\"inner\", on=\"id\")\n",
    "outlier_df = outlier_df.drop(\"text_y\", axis=1)\n",
    "outlier_df = outlier_df.sort_values(\"max\", ascending=False)\n",
    "outlier_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41cab2-46b0-425d-b214-4215f69ee450",
   "metadata": {},
   "source": [
    "### Impact of data size\n",
    "\n",
    "Since a large part of our data are of poor quality, we should  we can look at the progressive learning curve of our classifier's performance to gauge how many data can be discarded before performance suffers.\n",
    "\n",
    "We split the training data into $[10%, 20%, \\cdots, 100%]$ random splits, leaving the test set whole. We then train our best classifier in all of the successive splits and keep the accuracy metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a1551-ca47-4c9c-a383-002cbd775ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_splits = np.arange(start=0.1, stop=1.1, step=0.1)\n",
    "splits = [data_train.sample(int(np.ceil(data_train.shape[0]*i))) for i in len_splits]\n",
    "\n",
    "sizes = [len(split) for split in splits]\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815a661-a71c-4e7e-9bd4-85d936cdf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "print(\"Running classifier on multiple training splits...\")\n",
    "for split in tqdm(splits):\n",
    "    x_train = vectorizer.transform(split.text)\n",
    "    y_train = split.generated\n",
    "    model = clone(best_model)\n",
    "    model = model.fit(x_train, y_train)\n",
    "\n",
    "    train_preds = model.predict(x_train)\n",
    "    train_scores.append(f1_score(y_train, train_preds, average=\"macro\"))\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_scores.append(f1_score(y_test, test_preds, average=\"macro\"))\n",
    "\n",
    "train_scores = np.array(train_scores)\n",
    "test_scores = np.array(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3738ddb2-ce44-43c5-a4f0-a7a6a3799207",
   "metadata": {},
   "source": [
    "We now plot the accuracy metric (here Macro-F1 score for reasons outlined in the `Classifiers` section) in relation to the number of data points. \n",
    "\n",
    "A useful question would be how much data would we need to reach 100% test set accuracy. Since it's improbable that our classifier can reach this score with a reasonable amount of data we resort to using a regression plot on the test set accuracy metric. This has several caveats:\n",
    "* The linearity assumption made by the (linear) regression plot is almost never accurate in training/test curves. The curves are oftentimes logarithmic, rapidly improving when only few data points exist, and plateauing after a certain inflection point. For example, the performance of a classifier will improve more when we add 10 points from a previous dataset of 100, than if we add the same amount of points to a previous dataset of 10,000\n",
    "* The shape of the curve will angle the regression line upwards since the first splits will almost certainly be much \"lower\" than the rest\n",
    "\n",
    "Concluding, while we certainly can use a linear regression line to gauge the amount of data until we reach 100% test set accuracy, this prediction will almost certainly be overly optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34129f3a-afcd-4197-9df4-e3f14243f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "fig.suptitle(\"Dataset size impact on LLM detection\")\n",
    "fig.tight_layout(pad=3)\n",
    "\n",
    "# regular regplot\n",
    "sns.regplot(y=train_scores, \n",
    "            x=sizes, \n",
    "            label=\"train\", \n",
    "            color=\"blue\", \n",
    "            ax=ax1,\n",
    "            scatter_kws={\"s\": 30})\n",
    "sns.regplot(y=test_scores,\n",
    "            x=sizes,\n",
    "            label=\"test\",\n",
    "            color=\"orange\",\n",
    "            marker=\"*\", \n",
    "            ax=ax1,\n",
    "            scatter_kws={\"s\": 100})\n",
    "\n",
    "ax1.set_title(\"Regression plot on original splits\")\n",
    "ax1.set_ylabel(\"Macro F1 score\")\n",
    "ax1.set_xlabel(\"Dataset size (number of entries)\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# regplot with expanded xlim\n",
    "plt.xlim(0, 7500)\n",
    "sns.regplot(y=train_scores, \n",
    "            x=sizes, \n",
    "            label=\"train\", \n",
    "            color=\"blue\", \n",
    "            ax=ax2,\n",
    "            scatter_kws={\"s\": 30},\n",
    "            truncate=False)\n",
    "sns.regplot(y=test_scores,\n",
    "            x=sizes,\n",
    "            label=\"test\",\n",
    "            color=\"orange\",\n",
    "            marker=\"*\", \n",
    "            ax=ax2,\n",
    "            scatter_kws={\"s\": 100},\n",
    "            truncate=False)\n",
    "\n",
    "ax2.set_title(\"Regression plot on predicted splits\")\n",
    "ax2.set_ylabel(\"Macro F1 score\")\n",
    "ax2.set_xlabel(\"Dataset size (number of entries)\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "# save and show\n",
    "save_plot(\"dataset_size.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423fc3e-9f32-4e66-929b-073fe18b859a",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "\n",
    "We notice the following important patterns:\n",
    "* The training F1-score remains at 100% irrespective of dataset size. This can be explained by the very low mean similarity scores between the generated and human texts. \n",
    "* The testing F1-score improves for a dataset of size up to 1100, at which point it oscillates around 95% testing F1-score\n",
    "* The convergence point of 100% testing score happens at approximately 7000 data points. However, we can be certain this is a very optimistic estimate for the reasons outlined above.\n",
    "\n",
    "From these patterns we can make two hypotheses for our problem:\n",
    "\n",
    "**Hypothesis 1: Our classifier has \"solved\" the underlying problem**. The extremely successful results shown here indicate that the classifier has modeled the actual distribution of the data, and can thus solve it with almost perfect accuracy in unseen data.\n",
    "\n",
    "**Hypothesis 2: The training data do not represent the underlying problem**. In that case, the extremely successful results shown here are indicative of overfitting, where both training and test data feature a fundamentally different (and more easily solvable) distribution than real-life data. This is the most probable hypothesis, and can result from low-quality data and a non-representative sample.\n",
    "\n",
    "Of course, the only way to know which hypothesis holds with any degree of certainty is to expose our trained classifier to unseen operational data which were collected with a different method from ours (different models, prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362606bc-ea60-4574-9461-16a86232bcb5",
   "metadata": {},
   "source": [
    "### Leave-One-Out Cross Validation\n",
    "\n",
    "By using LOOV-CV we can hopefully correlate the prediction probability of every generated text with its mean and max similarity with human texts, as described above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaaeff0-e348-4e64-b403-c5bca68c83e0",
   "metadata": {},
   "source": [
    "#### Exporting intermediary data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe629692-36a2-4aa7-805f-05fef291edca",
   "metadata": {},
   "source": [
    "Since the LOOV procedure is **extremely** computationally expensive, we will offload it to [another notebook](loov.ipynb) to be executed separately (and much more sparingly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbaa8b-b723-49c1-ae68-8da8cc1dc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skops.io as sio\n",
    "\n",
    "\n",
    "print(\"Exporting model...\")\n",
    "sio.dump(best_model, os.path.join(notebook_config.INTERMEDIATE_DIR, \n",
    "                                  notebook_config.MODEL_FILE_NAME))\n",
    "print(\"Model exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009655b-9ac3-416e-90b6-71e73faf276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(notebook_config.INTERMEDIATE_DIR, \n",
    "                       notebook_config.LOOV_INPUT_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fbd3e-8b87-44a1-a41d-0daa45cf2802",
   "metadata": {},
   "source": [
    "#### Running LOOV analysis\n",
    "\n",
    "We retrieve the results of our LOOV. While the results do not include every single generated text due to computational cost, we do not need all texts. \n",
    "\n",
    "This section is about analyzing the correlation between mean/max text similarity and prediction probability, and thus we only need a representative sample, large enough for our conclusions to be statistically significant.\n",
    "\n",
    "More specifically, we are looking for data points that are likely to have low prediction probability, meaning that they sufficiently \"confuse\" the classifier. Those data points are much more valuable to us than clearly generated texts which the classifier can trivially distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a49bb-483b-4ca1-9f72-b22bbf9b5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(os.path.join(notebook_config.INTERMEDIATE_DIR, \n",
    "                                  notebook_config.LOOV_RES_NAME))\n",
    "res_df = res_df.drop(\"Unnamed: 0\", axis=1)\n",
    "res_df.id = res_df.id.astype(str)\n",
    "\n",
    "analysis_df = res_df.merge(similarity_df, \n",
    "                           how=\"inner\", \n",
    "                           on=\"text\", \n",
    "                           suffixes=(\"\", \"_x\"))\n",
    "analysis_df = analysis_df.drop(\"id_x\", axis=1)\n",
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cd6d1-36c4-4283-812c-3aae88c8c632",
   "metadata": {},
   "source": [
    "Normal statistical analysis in this case is going to be very challenging, since (as stated above) the distributions of the mean/max similarity are not normal and definitely not homoscedastic. Thus, most traditional statistical models would fail at accurately describing their relationship.\n",
    "\n",
    "Instead, we can draw conclusions graphically, if those are apparent enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13a1d6-9ccc-41a5-ae4e-8dc8094b6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"mean\", y=\"proba\", data=analysis_df)\n",
    "sns.regplot(x=\"max\", y=\"proba\", color=\"green\", data=analysis_df)\n",
    "\n",
    "plt.axhline(1, color=\"black\")\n",
    "plt.title(\"Generated essay diversity vs prediction probability\")\n",
    "plt.ylabel(\"Prediction probability\")\n",
    "plt.xlabel(\"Jaccard Similarity with all human essays\")\n",
    "plt.legend(title='Similarity', labels=['Mean', 'Max'], loc=\"lower center\")\n",
    "\n",
    "save_plot(\"diversity_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d1e4b0-14a2-48ec-b041-5c1989a8a232",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "We notice a slight inverse trend between max similarity and prediction probability. Obviously, when similarity becomes 1, or very close to 1, the classifier fails since the data point itself is mislabeled. There are a few valuable points, but most reside in an area of low similarity with human texts and very high prediction probability.\n",
    "\n",
    "The conclusions we draw from this graph therefore are:\n",
    "* Some of the generated essays are clearly copied by the LLM, and thus functionally useless.\n",
    "* Most points are irrelevant with regards to prediction probability\n",
    "* There is a slight inverse relationship between max similarity and prediction probability, meaning that, in general, essays that are more similar to human texts tend to be more valuable for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441318f-f1f8-4379-b741-8aabfa8ad30c",
   "metadata": {},
   "source": [
    "### Selecting the optimal training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c1545-eb2b-443d-bf15-15f29ef070b3",
   "metadata": {},
   "source": [
    "Based on the conclusions drawn from the analyses above, we can create an \"optimal\" dataset which comprises the most valuable generated texts. \n",
    "\n",
    "Quality in this case is defined by:\n",
    "* Any generated text which is separable with human texts (texts with at most 95% max similarity with human texts)\n",
    "* Generated texts with as high max similarity with human texts as possible. Since mean and max similarity are clearly positively correlated, selecting texts with high similarity means they also feature high mean similarity.\n",
    "\n",
    "We achieve this dataset by filtering out the non-separable texts and filling the new dataset with a subset of generated texts sorted by max similarity. We choose to generate a completely balanced dataset, since we don't know the underlying population and thus the a-priori probabilities of each class. We also balance our dataset in respect to the prompt used in order to not fit a classifier trained only on one of the possible essay subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd01951-4c28-48e4-95b3-a164d17e7581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove non-separable texts\n",
    "optimal_df = df.merge(similarity_df.loc[:, [\"id\", \"max\"]], how=\"outer\", on=\"id\")\n",
    "optimal_df = optimal_df[~(optimal_df[\"max\"] > 0.95)]\n",
    "optimal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75cc6a-3fe7-4d06-aa4d-17fb49f76bfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# balance dataset with highest quality essays\n",
    "human_len = df[df.generated==0].shape[0]\n",
    "\n",
    "new_generated_cars = optimal_df[(optimal_df.generated == 1)\n",
    "                                & (optimal_df.prompt_id == 0)\n",
    "                                ].sort_values(\"max\", ascending=False)[:human_len//2]\n",
    "new_generated_elect = optimal_df[(optimal_df.generated == 1) \n",
    "                                & (optimal_df.prompt_id == 1\n",
    "                                ].sort_values(\"max\", ascending=False)[:human_len//2]\n",
    "\n",
    "optimal_df = pd.concat([optimal_df[optimal_df.generated == 0], \n",
    "                        new_generated_cars,\n",
    "                        new_generated_elect], ignore_index=True)\n",
    "\n",
    "optimal_df = optimal_df.drop(\"max\", axis=1)\n",
    "optimal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017098b-fbb8-4cfe-8956-4f6fb876e84f",
   "metadata": {},
   "source": [
    "## Clustering-based augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa553d5-483a-4aa4-9790-d6d46b3ed95c",
   "metadata": {},
   "source": [
    "In order to determine the \"variance\" of our generated samples, we can use clustering techniques to determine in which groups the human and generated essays belong in. Using those groups we can extract their similarities by looking into their individual members, and determine which groups may be underrepresented in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9599db-9671-4de9-8add-3599d2417789",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "Before we execute the clustering algorithm we need to determine the number of clusters in our data. This can be either done by intuition (if we already know how many classes *should* exist according to our problem), or by statistical techniques.\n",
    "\n",
    "Our intuition would lead us to choose $2$ clusters for both generated and human essays, one for each prompt. However, we are interested in seeing if there are underlying patterns within those two large clusters, which is why we need to use the aforementioned techniques.\n",
    "\n",
    "There are two main methods we can use to determine the number of clusters, the **Elbow** method and the **Silhouette** score. To simplify, the Elbow method determines the last point at which the model improves significantly while the Silhouette score determines the cluster cohesion. \n",
    "\n",
    "We follow the advice of [this post](https://builtin.com/data-science/elbow-method), which warns against relying on the Elbow method when its results are not decisive (which is often the case), and thus use both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49711af-484c-466f-99e6-637a7904fd2e",
   "metadata": {},
   "source": [
    "### Clustering Generated Essays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8620bdf-e75c-43ad-98e9-61241cb0d948",
   "metadata": {},
   "source": [
    "#### Determining the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50869b92-04e0-4525-85d4-54a74fbebb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df = optimal_df[optimal_df.generated == 1]\n",
    "human_df = optimal_df[optimal_df.generated == 0]\n",
    "\n",
    "x_generated = vectorizer.transform(generated_df.text).toarray()\n",
    "x_human = vectorizer.transform(human_df.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da316721-1497-4731-9bdc-5e7452d179e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "\n",
    "# code for the graphs adapted from https://builtin.com/data-science/elbow-method\n",
    "\n",
    "# there is an internal memory leak here, don't worry about it\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    km = KMeans(random_state=42, n_init=5)\n",
    "    visualizer = KElbowVisualizer(km, k=(2,10))\n",
    "    visualizer.fit(x_generated)        \n",
    "    \n",
    "plt.title(\"Generated Essays - Distortion Score Elbow for KMeans with TF-IDF embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2253f9-c12e-4090-8c68-718671a6a9bd",
   "metadata": {},
   "source": [
    "While the Elbow method indicates that 4 clusters are optimal, there is no desicive \"elbow\" at any point in the graph. Thus, we will use the Silhouette method to hopefully get more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d562c6-68a5-4306-997b-4453164c9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15,8))\n",
    "\n",
    "print(\"Generating plots...\")\n",
    "for i in tqdm(range(2, 8)):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        km = KMeans(n_clusters=i, init='k-means++', n_init=5, random_state=42)\n",
    "        \n",
    "        q, mod = divmod(i, 2)\n",
    "        ax = axes[q-1][mod]\n",
    "        ax.set_title(f\"K={i}\")\n",
    "        \n",
    "        visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax)\n",
    "        visualizer.fit(x_generated) \n",
    "\n",
    "fig.suptitle(\"Generated Essays - Silhouette Plot of KMeans for using TF-IDF vectors\")\n",
    "fig.tight_layout(h_pad=3, w_pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203967b8-6045-4f2f-a968-01e8d28cbd8d",
   "metadata": {},
   "source": [
    "These graphs represent how similar the points in each cluster with each other compared to the ones from other clusters. The bigger the score, the more \"sure\" we are the point belongs in the cluster, while negative values indicate the point has probably been mis-clustered. Each \"blob\" represents a cluster and the red line is the mean Silhouette score of all clusters.\n",
    "\n",
    "We are looking for two main patterns in the graphs (we direct the reader to the [previous post](https://builtin.com/data-science/elbow-method) for details):\n",
    "* All clusters must pass through the red line (all clusters must be significant)\n",
    "* The cluster sizes should be as equal as possible.\n",
    "\n",
    "Taking into account the two criteria, the optimal clustering for generated essays is $K=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90899957-59ad-446d-a54d-0fb91914631a",
   "metadata": {},
   "source": [
    "#### Determining the clusters\n",
    "\n",
    "In order to qualitatetively assess the clustering we need to take a look at the clusters ourselves. The easiest and most reliable way is to print out a sample of each cluster and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979f19b-a2cc-4f69-b4ef-98e01063892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(clusters: np.ndarray, data_df: pd.DataFrame, n_samples: int=2) -> None:\n",
    "    \"\"\"\n",
    "    Print samples from each cluster along with the corresponding cluster label.\n",
    "\n",
    "    :param clusters: An array containing cluster labels.\n",
    "    :type clusters: array-like\n",
    "    :param data_df: A pandas DataFrame containing text data.\n",
    "    :type data_df: pd.DataFrame\n",
    "    :param n_samples: An integer specifying the number of samples to print for each cluster. Default is 2.\n",
    "    :type n_samples: int\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for cluster in np.unique(clusters):\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Cluster {cluster}\")\n",
    "        for text in data_df[clusters==cluster].sample(n_samples).text:\n",
    "            print(\"\\t\\t\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832373b7-20f8-4624-b259-e25de1c274e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, \n",
    "            init=\"k-means++\", \n",
    "            random_state=42, \n",
    "            n_init=10)\n",
    "generated_clusters = km.fit_predict(x_generated)\n",
    "print_clusters(generated_clusters, generated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9dd0f9-11d7-446d-82d2-9d3198049bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_cluster_mapping = {\n",
    "    0: \"Car-Free cities\", \n",
    "    1: \"Electoral College\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be880997-3c4a-40c3-a416-32296251b407",
   "metadata": {},
   "source": [
    "It looks like the essays have been clustered according to their prompt, which confirms our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729c279a-f2c4-4411-9f3c-53d87043cf3f",
   "metadata": {},
   "source": [
    "### Clustering Human Essays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4d453-5b69-42c6-aa07-35bd634b3b13",
   "metadata": {},
   "source": [
    "#### Determining the number of clusters\n",
    "\n",
    "We begin with the Elbow method, just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678858b-cea5-49d7-adc2-a785b73d3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    km = KMeans(random_state=42, n_init=5)\n",
    "    visualizer = KElbowVisualizer(km, k=(2,10))\n",
    "    visualizer.fit(x_human)        \n",
    "    \n",
    "plt.title(\"Human Essays - Distortion Score Elbow for KMeans\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abad901-5e72-42e3-aa25-1b79e9b8c09e",
   "metadata": {},
   "source": [
    "This is an even more perplexing elbow graph, so we turn to Silhouette score to get a better picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9991a84-63b8-4c86-b361-054441f8a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15,8))\n",
    "\n",
    "print(\"Generating plots...\")\n",
    "for i in tqdm(range(2, 8)):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        km = KMeans(n_clusters=i, init='k-means++', n_init=5, random_state=42)\n",
    "        \n",
    "        q, mod = divmod(i, 2)\n",
    "        ax = axes[q-1][mod]\n",
    "        ax.set_title(f\"K={i}\")\n",
    "        \n",
    "        visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax)\n",
    "        visualizer.fit(x_human) \n",
    "\n",
    "fig.suptitle(\"Human Essays - Silhouette Plot of KMeans for human essays with TF-IDF vectors\")\n",
    "fig.tight_layout(h_pad=3, w_pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a50ee-48cc-4281-9d7e-7f3aa09e775c",
   "metadata": {},
   "source": [
    "While we could accept $K=2$ for the same merits as in the generated essays, we must acknowledge that the clustering is much more difficult in our case. A probable reason for that is that the TF-IDF representation on which the clustering is based on, is insufficient for the human essays, either due to its shape in the $l$ dimensional space, or because of sparsity.\n",
    "\n",
    "A way to circumvent this limitation is swapping the text representation to the Word2Vec embeddings. While probably insufficient for classification, their low dimensionality and their much higher similarity with each other (which was determined in a previous version of this notebook) make them suitable candidates for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73217b5a-ccfe-4ce1-8168-22cdec31365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_human = np.array([x for x in optimal_df.loc[optimal_df.generated==0].embedding])\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15,8))\n",
    "\n",
    "print(\"Generating plots...\")\n",
    "for i in tqdm(range(2, 8)):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        km = KMeans(n_clusters=i, init='k-means++', n_init=5, random_state=42)\n",
    "        \n",
    "        q, mod = divmod(i, 2)\n",
    "        ax = axes[q-1][mod]\n",
    "        ax.set_title(f\"K={i}\")\n",
    "        \n",
    "        visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax)\n",
    "        visualizer.fit(embed_human) \n",
    "\n",
    "fig.suptitle(\"Human Essays - Silhouette Plot of KMeans using Word2Vec embeddings\")\n",
    "fig.tight_layout(h_pad=3, w_pad=3) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66c017e-405d-4ee4-bb1d-0bcf029b2e5b",
   "metadata": {},
   "source": [
    "This is a much clearer graph, carrying very important information; unlike the generated clusters, the human essays seem to feature underlying patterns in their 2 main clusters, making the optimal clustering $K=4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1822d-b034-418b-97ed-898fa67b7788",
   "metadata": {},
   "source": [
    "#### Determining the clusters\n",
    "\n",
    "We follow the same procedure as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb19b76-f305-4078-89fe-f28bd432fdbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4, \n",
    "                init=\"k-means++\", \n",
    "                random_state=42, \n",
    "                n_init=10)\n",
    "km = km.fit(embed_human)\n",
    "human_clusters = km.predict(embed_human)\n",
    "print_clusters(human_clusters, human_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfdda7-4c62-4510-b400-d3004201af86",
   "metadata": {},
   "source": [
    "Clusters 0 and 3 seem to feature prompts about car-free cities, while clusters 1 and 2 about the electoral college, corresponding to clusters 0 and 1 of the generated essay clusters respectively. \n",
    "\n",
    "The most notable difference between the clusters featuring the same prompts is that clusters 0 and 1 appear to stick closely to sources, citing them explicitly and often, and being much more likely to use text found in the sources themselves. Thus, we can yield a title for each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bf58c-bc39-4a3d-aa59-ed368938634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_mapping = {0: \"Cars w/sources\", \n",
    "                   1: \"Election w/sources\",\n",
    "                   2: \"Election no sources\",\n",
    "                   3: \"Cars no sources\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471b3df-2fe2-4578-9761-96e177364ca5",
   "metadata": {},
   "source": [
    "### Relating the two clusterings\n",
    "\n",
    "By printing some representatives from each clustering, it's clear that both are broadly split in regards to the prompt, one cluster belonging to the Electoral College and the other to Car-Free cities. However, human essays are also split in two sub-clusters for each prompt, which can be seen both from the Silhouette graphs, and the printed samples above.\n",
    "\n",
    "Thus, it's easy to conclude that $Cluster_{generated,0} \\rightarrow \\{Cluster_{human, 0}, Cluster_{human, 3}\\}$ and $Cluster_{generated,1} \\rightarrow \\{Cluster_{human, 1}, Cluster_{human, 2}\\}$. While we could use a hierarchical clustering algorithm to confirm, our results are intuitive, clearly represented in the printed representatives, and justified by the Silhouette plots. Thus, we can rely on this qualitative conclusion without utilizing quantitative metrics (such as mean distance between clusterings) and taking the risk of False-Negative results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9121115-6ea0-4298-8cf4-decf5ffd9604",
   "metadata": {},
   "source": [
    "### Determining the balance of the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208af9d-054b-41f8-b5a3-e644743dd87a",
   "metadata": {},
   "source": [
    "In order to get a faithful representation of the student essays by our generated dataset, we should ensure that the generated essays correspond roughly proportionally to the student clusters. \n",
    "\n",
    "We will cluster the generated essays according to the K-means model we trained on human essays and compare the number of essays in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed55d7-9e5f-4730-83e5-880c29b95336",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_generated = np.array([x for x in optimal_df.loc[optimal_df.generated==1].embedding])\n",
    "guessed_gen_clusters = km.predict(embed_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e4b6b-a9a6-41b7-9b30-66078eaba743",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame({\"cluster\": np.concatenate([\n",
    "                                guessed_gen_clusters, \n",
    "                                human_clusters]),\n",
    "                          \"generated\": np.concatenate([\n",
    "                              np.ones_like(guessed_gen_clusters), \n",
    "                              np.zeros_like(human_clusters)])})\n",
    "\n",
    "cluster_df.cluster = cluster_df.cluster.map(cluster_mapping)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4480ac-b9d6-4213-af17-c54586cd6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "cluster_human_df = cluster_df[cluster_df.generated == 0]\n",
    "cluster_human_df.cluster.value_counts().sort_index().plot(\n",
    "    kind='bar', \n",
    "    color=['blue', 'green', 'red', 'purple'],\n",
    "    ax=ax1)\n",
    "ax1.set_title(\"Human Essays\")\n",
    "ax1.set_xlabel(\"Cluster\")\n",
    "ax1.set_ylabel(\"Number of essays\")\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
    "\n",
    "\n",
    "gen_human_df = cluster_df[cluster_df.generated == 1]\n",
    "gen_human_df.cluster.value_counts().sort_index().plot(\n",
    "    kind='bar', \n",
    "    color=['blue', 'green', 'red', 'purple'],\n",
    "    ax=ax2)\n",
    "ax2.set_title(\"Generated Essays\")\n",
    "ax2.set_xlabel(\"Cluster\")\n",
    "ax2.set_ylabel(\"Number of essays\")\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n",
    "\n",
    "fig.suptitle(\"Number of essays per cluster\")\n",
    "save_plot(\"clusters.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5715db-2f60-4a33-934f-081e6a61450a",
   "metadata": {},
   "source": [
    "### Dataset augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b85336-3e30-4b4a-ba95-6915874ed7bf",
   "metadata": {},
   "source": [
    "We have determined that the most important weakness in our dataset is the lack of generated essays which follow the sources closely. We prompt ChatGPT accordingly and add its responses to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6a9ee-4c92-493e-9dbd-06aeeb491317",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_cars_df = create_dataset(read_gpt(\"cluster_augmentation_cars.md\"), \n",
    "                               prompt_id=0, \n",
    "                               llm=\"ChatGPT\", \n",
    "                               source=\"Dimitris Tsirmpas\")\n",
    "\n",
    "extra_elect_df = create_dataset(read_gpt(\"cluster_augmentation_election.md\"), \n",
    "                               prompt_id=1, \n",
    "                               llm=\"ChatGPT\", \n",
    "                               source=\"Dimitris Tsirmpas\")\n",
    "extra_df = pd.concat([extra_cars_df, extra_elect_df], ignore_index=True)\n",
    "extra_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf1160-d546-45bf-a2df-31df593ec812",
   "metadata": {},
   "source": [
    "### Training the best classifier with the augmented data\n",
    "\n",
    "We can now retrain our best classifier with the extra data and check whether our more balanced dataset helps improve its performace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c9f8e-2283-43a4-b12f-d4f315b0960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df_prepr = extra_df.copy()\n",
    "extra_df_prepr.text = preprocess(extra_df_prepr.text)\n",
    "extra_df_prepr[\"embedding\"] = compute_embeddings(extra_df_prepr.text) \n",
    "\n",
    "augmented_df_prepr = pd.concat([extra_df_prepr, optimal_df])\n",
    "augmented_df_prepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74799b4-9959-41ba-b6ca-b4503a8e60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(augmented_df_prepr,\n",
    "                                        train_size=0.7,\n",
    "                                        test_size=0.3,\n",
    "                                         random_state=42)\n",
    "data = get_data_from_text(vectorizer, data_train, data_test)\n",
    "\n",
    "final_x_train = data[\"x_train\"]\n",
    "final_y_train = data[\"y_train\"]\n",
    "final_x_test = data[\"x_test\"]\n",
    "final_y_test = data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83a60a-6aca-4c68-874d-831274c73513",
   "metadata": {},
   "source": [
    "Since we fundementally changed the training dataset we need to run a new hyper-parameter search for our classifier. We do not need to consider other classifiers however, since both datasets originate from the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c020a7-3f04-4e0f-a471-98adea80319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(final_x_train, final_y_train)\n",
    "final_best_model = search.best_estimator_\n",
    "final_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a89dd-c664-4d09-8094-dcb05f57c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_res(final_best_model, final_x_train, final_y_train)\n",
    "print(f\"Best model with optimized dataset Macro-F1:\\n mean:{res[0]:.4f}, std: {res[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac578f7-f8ae-4ca3-8148-a611a3ca18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = final_best_model.fit(final_x_train, final_y_train)\n",
    "preds = final_best_model.predict(final_x_test)\n",
    "\n",
    "print(classification_report(final_y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40e43c-84a0-42f4-939f-cf28a32f2947",
   "metadata": {},
   "source": [
    "We can see that our classifier has noticebly improved, as did its training time.\n",
    "\n",
    "Additonally, we can be more optimistic about its ability to generalize on the operational dataset, since this score was achieved after getting rid of most low-quality datapoints. Many of these data points included \"low-hanging fruit\", or in other words texts that had no similarity with any human text, and thus served to pad the classifier's accuracy upwards as well as most likely overfitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec85f37-3586-4fb3-84ce-623b558d1490",
   "metadata": {},
   "source": [
    "## Exporting the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66580817-e7e0-4853-8f1a-440528ff3f48",
   "metadata": {},
   "source": [
    "The full dataset comprises the (few) generated prompts provided to us as well as all the generated prompts we generated (even if only a subset was used for the purposes of this notebook).\n",
    "\n",
    "Asides from meta-information included in the original dataset we also include the \"model\" and \"source\" columns defined earlier, as well as a column denoting whether the text was deemed of enough quality to be used in the training dataset. Finally, we add a column representing the cluster of each essay according to the K-Means model fitted on the human essays.\n",
    "\n",
    "The final, formatted augmentation dataset can be found at [output/augmentation.csv](output/augmentation.csv). The file is structured as follows:\n",
    "\n",
    "| Column | Type | Description  |\n",
    "|---|---|---|\n",
    "| id  | string | A unique identifier for the essay  |\n",
    "| text  | string | The text of the essay  |\n",
    "|  prompt_id | integer |  Which prompt the essay was generated from, relates to [data/train_prompts.csv](data/train_prompts.csv) |\n",
    "| generated | integer | Whether the essay was generated by an LLM, all 1  |\n",
    "| cluster |  integer | The assigned cluster of the text, as described in the notebook |  \n",
    "| LLM |  string | The LLM which generated the essay |\n",
    "| source |  string | Dataset attribution |\n",
    "| gold | integer | Whether the dataset was used in the final training phase of the LLM detector. These essays are considered the highest quality amongst the dataset, as explained in the notebook |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a39eb-859e-4d03-8b90-4251b3ac935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([extra_cars_df, original_df[original_df.generated==1]])\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b45c1-0759-408a-8542-1d7ccb3248da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Determining clusters:\")\n",
    "print(\"\\tPreprocessing...\")\n",
    "full_text = preprocess(full_df.text)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "print(\"\\t\", end=\"\")\n",
    "full_embeddings = compute_embeddings(full_text)\n",
    "print(\"\\tRunning K-Means...\")\n",
    "clusters = km.predict(np.array([x for x in full_embeddings]))\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "full_df[\"cluster\"] = clusters\n",
    "full_df.cluster = full_df.cluster.map(cluster_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003fcda-d25c-4a51-8bb0-198635e4fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retrieving training data...\")\n",
    "mask = [optimal_df.id.str.contains(index).any() for index in full_df.id]\n",
    "full_df[\"gold\"] = np.where(mask, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d3677-aea8-4d1d-8d42-6df96e9daa85",
   "metadata": {},
   "source": [
    "We will also build a brief graph summarizing the key characteristics of our whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86617384-681b-4124-9236-5424c50637e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_SIZE = 8\n",
    "\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "fig.tight_layout(pad=3.0)\n",
    "fig.suptitle(\"Augmentation Dataset Statistics\")\n",
    "\n",
    "axes[0, 0].pie(full_df.prompt_id.value_counts(), \n",
    "        labels=[\"Car-Free cities\", \"Electoral College\"], \n",
    "        textprops={'fontsize': TEXT_SIZE},\n",
    "        colors=[\"orange\", \"green\"],\n",
    "        autopct=f'%1.1f%%')\n",
    "axes[0, 0].set_title(\"Prompts\")\n",
    "\n",
    "axes[0, 1].pie(full_df.cluster.value_counts(), \n",
    "        labels=cluster_mapping.values(), \n",
    "        textprops={'fontsize': TEXT_SIZE},\n",
    "        autopct=f'%1.1f%%')\n",
    "axes[0, 1].set_title(\"Category\")\n",
    "\n",
    "axes[1, 0].pie(full_df.source.value_counts(), \n",
    "        labels=[\"PaLM (external source)\", \"PaLM(project dataset)\", \"ChatGPT\", \"Unknown\"], \n",
    "        textprops={'fontsize': TEXT_SIZE},\n",
    "        colors=[\"gray\", \"lightblue\", \"red\", \"black\"],\n",
    "        autopct=f'%1.1f%%')\n",
    "axes[1, 0].set_title(\"LLM\")\n",
    "\n",
    "axes[1, 1].pie(full_df.gold.value_counts(), \n",
    "        labels=[\"Not used\", \"Used\"], \n",
    "        textprops={'fontsize': TEXT_SIZE},\n",
    "        colors=[\"red\", \"lightblue\"],\n",
    "        autopct=f'%1.1f%%')\n",
    "axes[1, 1].set_title(\"Used in training\")\n",
    "\n",
    "\n",
    "save_plot(\"augmentation_stats.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147767d4-b718-46bf-a91a-d5911eb90162",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_output(full_df, \"augmentation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfef77-dd1a-4efd-88e6-336b190efc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Notebook executed in {int((time()-start)// 60)} minutes and {(time()-start) % 60:.1f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
